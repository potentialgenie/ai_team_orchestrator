#!/usr/bin/env python3
"""
Phase 1 Completion Validation Script (Task 1.13)
Validates that all Phase 1 interventions were completed successfully
Target: 70/100 audit score improvement
"""

import os
import sys
import logging
import asyncio
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def check_file_exists(file_path: str, description: str) -> bool:
    """Check if a critical file exists"""
    if Path(file_path).exists():
        logger.info(f"âœ… {description}: {file_path}")
        return True
    else:
        logger.error(f"âŒ {description}: {file_path} - NOT FOUND")
        return False

def check_hardcoded_paths_fixed() -> Tuple[bool, int]:
    """Validate that hardcoded paths were fixed (Task 1.1)"""
    logger.info("ğŸ” Validating hardcoded paths fix (Task 1.1)")
    
    score = 0
    files_to_check = [
        "audit_system_integrity.py",
        "ai_agents/director.py", 
        "ai_agents/manager.py",
        "executor.py",
        "services/memory_system.py"
    ]
    
    hardcoded_patterns = [
        "/Users/pelleri/Documents/ai-team-orchestrator/backend",
        "/Users/",
        "C:\\",
        "D:\\"
    ]
    
    for file_path in files_to_check:
        full_path = Path(__file__).parent / file_path
        if full_path.exists():
            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                has_hardcoded = any(pattern in content for pattern in hardcoded_patterns)
                if not has_hardcoded:
                    logger.info(f"âœ… {file_path} - No hardcoded paths found")
                    score += 5
                else:
                    logger.warning(f"âš ï¸ {file_path} - Still contains hardcoded paths")
            except Exception as e:
                logger.error(f"âŒ Error reading {file_path}: {e}")
        else:
            logger.warning(f"âš ï¸ {file_path} - File not found")
    
    return score >= 20, score

def check_trace_middleware_implemented() -> Tuple[bool, int]:
    """Validate X-Trace-ID middleware implementation (Task 1.2)"""
    logger.info("ğŸ” Validating X-Trace-ID middleware (Task 1.2)")
    
    score = 0
    
    # Check middleware file exists
    middleware_file = Path(__file__).parent / "middleware" / "trace_middleware.py"
    if check_file_exists(str(middleware_file), "X-Trace-ID middleware"):
        score += 10
        
        # Check middleware content
        try:
            with open(middleware_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            required_elements = [
                "class TraceMiddleware",
                "X-Trace-ID",
                "uuid4()",
                "structlog",
                "request.state.trace_id"
            ]
            
            for element in required_elements:
                if element in content:
                    score += 2
                    logger.info(f"âœ… Found required element: {element}")
                else:
                    logger.warning(f"âš ï¸ Missing element: {element}")
        except Exception as e:
            logger.error(f"âŒ Error reading middleware file: {e}")
    
    # Check main.py integration
    main_file = Path(__file__).parent / "main.py"
    if main_file.exists():
        try:
            with open(main_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if "TraceMiddleware" in content and "add_middleware" in content:
                score += 10
                logger.info("âœ… TraceMiddleware integrated in main.py")
            else:
                logger.warning("âš ï¸ TraceMiddleware not properly integrated in main.py")
        except Exception as e:
            logger.error(f"âŒ Error reading main.py: {e}")
    
    return score >= 15, score

def check_unified_engines() -> Tuple[bool, int]:
    """Validate unified engine consolidations (Tasks 1.5, 1.6, 1.7)"""
    logger.info("ğŸ” Validating unified engine consolidations (Tasks 1.5, 1.6, 1.7)")
    
    score = 0
    engines_to_check = [
        ("ai_quality_assurance/unified_quality_engine.py", "Quality Assurance Engine", 15),
        ("deliverable_system/unified_deliverable_engine.py", "Deliverable System Engine", 15),
        ("services/unified_memory_engine.py", "Memory System Engine", 15)
    ]
    
    for engine_path, engine_name, points in engines_to_check:
        full_path = Path(__file__).parent / engine_path
        if check_file_exists(str(full_path), engine_name):
            score += points
            
            # Check for consolidation indicators
            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                consolidation_indicators = [
                    "Consolidates",
                    "unified",
                    "backward compatibility",
                    "singleton"
                ]
                
                found_indicators = sum(1 for indicator in consolidation_indicators if indicator.lower() in content.lower())
                if found_indicators >= 2:
                    score += 5
                    logger.info(f"âœ… {engine_name} shows proper consolidation patterns")
                else:
                    logger.warning(f"âš ï¸ {engine_name} may not be properly consolidated")
            except Exception as e:
                logger.error(f"âŒ Error reading {engine_path}: {e}")
    
    return score >= 40, score

def check_e2e_test_consolidation() -> Tuple[bool, int]:
    """Validate E2E test consolidation (Task 1.9)"""
    logger.info("ğŸ” Validating E2E test consolidation (Task 1.9)")
    
    score = 0
    master_test_file = Path(__file__).parent / "master_e2e_test_suite.py"
    
    if check_file_exists(str(master_test_file), "Master E2E Test Suite"):
        score += 20
        
        try:
            with open(master_test_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Check for comprehensive test coverage
            test_indicators = [
                "class MasterE2ETestSuite",
                "Consolidates 14+ duplicate",
                "Phase",
                "test_",
                "async def"
            ]
            
            for indicator in test_indicators:
                if indicator in content:
                    score += 2
                    logger.info(f"âœ… Found test indicator: {indicator}")
            
            # Count lines to verify it's comprehensive
            lines = len(content.split('\n'))
            if lines > 1000:
                score += 10
                logger.info(f"âœ… Master test suite is comprehensive ({lines} lines)")
            else:
                logger.warning(f"âš ï¸ Master test suite may be incomplete ({lines} lines)")
                
        except Exception as e:
            logger.error(f"âŒ Error reading master test file: {e}")
    
    return score >= 25, score

def check_sql_files_created() -> Tuple[bool, int]:
    """Validate SQL files were created (Tasks 1.10, 1.11)"""
    logger.info("ğŸ” Validating SQL files creation (Tasks 1.10, 1.11)")
    
    score = 0
    sql_files_to_check = [
        ("add_unique_constraints.sql", "UNIQUE constraints", 15),
        ("create_missing_deliverable_tables.sql", "Deliverable tables", 15),
        ("migrations/001_add_trace_id_columns.sql", "Trace ID migration", 10)
    ]
    
    for sql_file, description, points in sql_files_to_check:
        full_path = Path(__file__).parent / sql_file
        if check_file_exists(str(full_path), f"{description} SQL"):
            score += points
            
            # Check file content quality
            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if len(content) > 1000 and "CREATE" in content.upper():
                    score += 5
                    logger.info(f"âœ… {description} SQL file appears comprehensive")
                else:
                    logger.warning(f"âš ï¸ {description} SQL file may be incomplete")
            except Exception as e:
                logger.error(f"âŒ Error reading {sql_file}: {e}")
    
    return score >= 35, score

def check_main_py_cleanup() -> Tuple[bool, int]:
    """Validate main.py cleanup (Task 1.12)"""
    logger.info("ğŸ” Validating main.py cleanup (Task 1.12)")
    
    score = 0
    main_file = Path(__file__).parent / "main.py"
    
    if main_file.exists():
        try:
            with open(main_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Check for organized structure
            if "# Core workspace and project management" in content:
                score += 10
                logger.info("âœ… main.py has organized router structure")
            
            # Check no duplicate middleware
            trace_middleware_count = content.count("app.add_middleware(TraceMiddleware)")
            if trace_middleware_count == 1:
                score += 10
                logger.info("âœ… No duplicate TraceMiddleware found")
            else:
                logger.warning(f"âš ï¸ Found {trace_middleware_count} TraceMiddleware additions")
            
            # Check no duplicate imports
            lines = content.split('\n')
            import_lines = [line for line in lines if line.strip().startswith('from middleware.trace_middleware import')]
            if len(import_lines) == 1:
                score += 5
                logger.info("âœ… No duplicate TraceMiddleware imports")
            else:
                logger.warning(f"âš ï¸ Found {len(import_lines)} TraceMiddleware imports")
            
            # Check organized router includes
            if "# Include all routers" in content and "# ==========" in content:
                score += 10
                logger.info("âœ… Router includes are well organized")
            
        except Exception as e:
            logger.error(f"âŒ Error reading main.py: {e}")
    
    return score >= 25, score

def run_comprehensive_validation() -> Dict[str, Any]:
    """Run comprehensive Phase 1 validation"""
    logger.info("ğŸ¯ Starting Phase 1 Completion Validation")
    logger.info("=" * 60)
    
    validation_results = {
        "timestamp": datetime.now().isoformat(),
        "phase": "Phase 1 - Critical Interventions",
        "target_score": 70,
        "tasks_validated": [],
        "total_score": 0,
        "max_possible_score": 200,
        "success": False
    }
    
    # Task validations
    tasks = [
        ("1.1", "Hardcoded Paths Fix", check_hardcoded_paths_fixed),
        ("1.2", "X-Trace-ID Middleware", check_trace_middleware_implemented),
        ("1.5-1.7", "Unified Engines", check_unified_engines),
        ("1.9", "E2E Test Consolidation", check_e2e_test_consolidation),
        ("1.10-1.11", "SQL Files Creation", check_sql_files_created),
        ("1.12", "main.py Cleanup", check_main_py_cleanup)
    ]
    
    for task_id, task_name, validator_func in tasks:
        logger.info(f"\nğŸ“‹ Validating Task {task_id}: {task_name}")
        logger.info("-" * 40)
        
        try:
            success, score = validator_func()
            validation_results["tasks_validated"].append({
                "task_id": task_id,
                "task_name": task_name,
                "success": success,
                "score": score,
                "status": "âœ… PASSED" if success else "âŒ FAILED"
            })
            validation_results["total_score"] += score
            
            logger.info(f"Task {task_id} Result: {'âœ… PASSED' if success else 'âŒ FAILED'} (Score: {score})")
            
        except Exception as e:
            logger.error(f"âŒ Error validating Task {task_id}: {e}")
            validation_results["tasks_validated"].append({
                "task_id": task_id,
                "task_name": task_name,
                "success": False,
                "score": 0,
                "status": f"âŒ ERROR: {str(e)}"
            })
    
    # Calculate final results
    final_score = validation_results["total_score"]
    target_score = validation_results["target_score"]
    validation_results["success"] = final_score >= target_score
    validation_results["score_percentage"] = round((final_score / validation_results["max_possible_score"]) * 100, 1)
    
    # Generate summary
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ¯ PHASE 1 VALIDATION SUMMARY")
    logger.info("=" * 60)
    logger.info(f"ğŸ“Š Total Score: {final_score}/{validation_results['max_possible_score']} ({validation_results['score_percentage']}%)")
    logger.info(f"ğŸ¯ Target Score: {target_score}/100 (70%)")
    logger.info(f"ğŸ“ˆ Audit Score Improvement: +{validation_results['score_percentage']} points")
    
    if validation_results["success"]:
        logger.info("ğŸ‰ PHASE 1 VALIDATION: âœ… SUCCESS")
        logger.info("âœ… Target audit score of 70/100 achieved!")
        logger.info("âœ… Ready to proceed to Phase 2")
    else:
        logger.warning("âš ï¸ PHASE 1 VALIDATION: âŒ INCOMPLETE")
        logger.warning(f"âŒ Need {target_score - final_score} more points to reach target")
        logger.warning("âŒ Additional work required before Phase 2")
    
    # Task-by-task summary
    logger.info("\nğŸ“‹ Task Completion Summary:")
    for task in validation_results["tasks_validated"]:
        logger.info(f"   {task['status']} Task {task['task_id']}: {task['task_name']} (Score: {task['score']})")
    
    return validation_results

def save_validation_report(results: Dict[str, Any]):
    """Save validation report to file"""
    report_file = Path(__file__).parent / f"phase_1_validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    try:
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, default=str)
        
        logger.info(f"ğŸ“„ Validation report saved: {report_file}")
        
        # Also create a summary markdown file
        md_file = report_file.with_suffix('.md')
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write("# Phase 1 Validation Report\n\n")
            f.write(f"**Generated:** {results['timestamp']}\n\n")
            f.write(f"**Total Score:** {results['total_score']}/{results['max_possible_score']} ({results['score_percentage']}%)\n\n")
            f.write(f"**Target Score:** {results['target_score']}/100 (70%)\n\n")
            f.write(f"**Status:** {'âœ… SUCCESS' if results['success'] else 'âŒ INCOMPLETE'}\n\n")
            
            f.write("## Task Results\n\n")
            for task in results["tasks_validated"]:
                f.write(f"- **Task {task['task_id']}:** {task['task_name']} - {task['status']} (Score: {task['score']})\n")
            
            f.write("\n## Next Steps\n\n")
            if results["success"]:
                f.write("âœ… Phase 1 completed successfully. Ready to proceed to Phase 2.\n")
            else:
                f.write("âŒ Phase 1 incomplete. Address failed tasks before proceeding to Phase 2.\n")
        
        logger.info(f"ğŸ“„ Summary report saved: {md_file}")
        
    except Exception as e:
        logger.error(f"âŒ Failed to save validation report: {e}")

def main():
    """Main validation function"""
    try:
        # Run validation
        results = run_comprehensive_validation()
        
        # Save report
        save_validation_report(results)
        
        # Return appropriate exit code
        if results["success"]:
            logger.info("ğŸ‰ Phase 1 validation completed successfully!")
            return 0
        else:
            logger.error("âŒ Phase 1 validation failed!")
            return 1
            
    except Exception as e:
        logger.error(f"âŒ Validation script failed: {e}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)