"""
AI Quality Gate Engine - Automated quality assurance system (Pillar 8: Quality Gates + Human-in-the-Loop)
Provides AI-driven quality validation, human review coordination, and automated approval workflows.
"""

import os
import json
import asyncio
import logging
from typing import List, Dict, Any, Optional, Tuple
from uuid import UUID, uuid4
from datetime import datetime, timedelta
from openai import AsyncOpenAI
from models import (
    AssetArtifact, QualityValidation, QualityRule, 
    EnhancedWorkspaceGoal, WorkspaceGoal
)
from database import (
    get_quality_rules_for_asset_type, log_quality_validation,
    update_artifact_status, get_artifacts_for_requirement,
    update_goal_progress
)

logger = logging.getLogger(__name__)

class AIQualityGateEngine:
    """AI-driven quality gate engine with human-in-the-loop (Pillar 8: Quality Gates)"""
    
    def __init__(self, openai_client: AsyncOpenAI = None):
        self.openai_client = openai_client or AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # Configuration from environment (Pillar-compliant)
        self.quality_validation_model = os.getenv("AI_QUALITY_VALIDATION_MODEL", "gpt-4o-mini")
        self.human_review_threshold = float(os.getenv("HUMAN_REVIEW_THRESHOLD", "0.7"))
        self.auto_enhancement_enabled = os.getenv("AUTO_ENHANCEMENT_ENABLED", "true").lower() == "true"
        self.quality_gate_timeout = int(os.getenv("QUALITY_GATE_TIMEOUT_SECONDS", "30"))
        self.min_quality_score = float(os.getenv("MIN_QUALITY_SCORE_FOR_APPROVAL", "0.8"))
        
        # Auto-learning configuration (Pillar 4: Auto-apprendente)
        self.auto_learning_enabled = os.getenv("ENABLE_AUTO_LEARNING_QUALITY_RULES", "true").lower() == "true"
        self.learning_feedback_loop = os.getenv("LEARNING_FEEDBACK_LOOP", "true").lower() == "true"
        
        # Performance monitoring (Pillar 11: Production-ready)
        self.performance_monitoring = os.getenv("ENABLE_PERFORMANCE_MONITORING", "true").lower() == "true"
        self.error_recovery_enabled = os.getenv("ENABLE_ERROR_RECOVERY", "true").lower() == "true"
        
        logger.info("ðŸ›¡ï¸ AIQualityGateEngine initialized with comprehensive quality assurance")
        
    async def validate_artifact_quality(self, artifact: AssetArtifact) -> Dict[str, Any]:
        """Comprehensive AI-driven quality validation (Pillar 8: Quality Gates)"""
        
        try:
            logger.info(f"ðŸ›¡ï¸ Starting quality validation for artifact: {artifact.artifact_name}")
            
            # Get applicable quality rules
            quality_rules = await get_quality_rules_for_asset_type(artifact.artifact_type)
            
            if not quality_rules:
                logger.warning(f"No quality rules found for asset type: {artifact.artifact_type}")
                return await self._fallback_quality_assessment(artifact)
            
            # Run validation against each rule
            validation_results = []
            for rule in quality_rules:
                try:
                    result = await self._execute_quality_rule(artifact, rule)
                    validation_results.append(result)
                    
                    # Log validation result
                    await log_quality_validation(result)
                    
                except Exception as e:
                    logger.error(f"Quality rule {rule.id} failed: {e}")
                    continue
            
            # Aggregate results and make decision
            quality_decision = await self._make_quality_decision(artifact, validation_results)
            
            # Update artifact status based on decision
            await self._apply_quality_decision(artifact, quality_decision)
            
            # Auto-learning: Update quality rules based on results (Pillar 4: Auto-apprendente)
            if self.auto_learning_enabled:
                await self._update_quality_rules_from_feedback(validation_results)
            
            logger.info(f"âœ… Quality validation completed - Decision: {quality_decision['status']}")
            return quality_decision
            
        except Exception as e:
            logger.error(f"Quality validation failed for artifact {artifact.id}: {e}")
            return {"status": "error", "error": str(e), "requires_human_review": True}
    
    async def _execute_quality_rule(self, artifact: AssetArtifact, rule: QualityRule) -> QualityValidation:
        """Execute a single quality rule against an artifact"""
        
        try:
            # Build comprehensive validation prompt
            validation_prompt = self._build_quality_validation_prompt(artifact, rule)
            
            # Use OpenAI SDK for validation (Pillar 1: OpenAI SDK)
            response = await self.openai_client.chat.completions.create(
                model=self.quality_validation_model,
                messages=[{"role": "user", "content": validation_prompt}],
                response_format={"type": "json_object"},
                temperature=0.1,  # Very low temperature for consistent validation
                timeout=self.quality_gate_timeout
            )
            
            validation_data = json.loads(response.choices[0].message.content)
            
            # Get workspace_id from artifact if available
            workspace_id = getattr(artifact, 'workspace_id', None)
            
            # Create validation record
            validation = QualityValidation(
                id=uuid4(),
                artifact_id=artifact.id,
                rule_id=rule.id,
                workspace_id=workspace_id,
                
                # Core validation results
                score=float(validation_data.get("quality_score", 0.0)),
                passed=validation_data.get("validation_passed", False),
                feedback=validation_data.get("detailed_feedback", ""),
                
                # AI insights and suggestions
                ai_assessment=validation_data.get("ai_assessment", ""),
                improvement_suggestions=validation_data.get("improvement_suggestions", []),
                
                # Business impact analysis  
                business_impact=validation_data.get("business_impact_analysis", ""),
                actionability_assessment=validation_data.get("actionability_score", 0.0),
                
                # Quality dimensions
                quality_dimensions={
                    "completeness": validation_data.get("completeness_score", 0.0),
                    "accuracy": validation_data.get("accuracy_score", 0.0),
                    "relevance": validation_data.get("relevance_score", 0.0),
                    "usability": validation_data.get("usability_score", 0.0),
                    "professional_standard": validation_data.get("professional_standard_score", 0.0)
                },
                
                # Metadata
                validation_model=self.quality_validation_model,
                validated_at=datetime.utcnow(),
                processing_time_ms=validation_data.get("processing_time_ms", 0),
                
                # Pillar compliance
                ai_driven=True,
                pillar_compliance_check=validation_data.get("pillar_compliance", {})
            )
            
            return validation
            
        except Exception as e:
            logger.error(f"Quality rule execution failed for rule {rule.id}: {e}")
            # Get workspace_id from artifact if available
            workspace_id = getattr(artifact, 'workspace_id', None)
            
            # Return failed validation
            return QualityValidation(
                id=uuid4(),
                artifact_id=artifact.id,
                rule_id=rule.id,
                workspace_id=workspace_id,
                score=0.0,
                passed=False,
                feedback=f"Validation failed due to system error: {str(e)}",
                ai_assessment="System error during validation",
                validation_model=self.quality_validation_model,
                validated_at=datetime.utcnow(),
                ai_driven=True
            )
    
    def _build_quality_validation_prompt(self, artifact: AssetArtifact, rule: QualityRule) -> str:
        """Build comprehensive quality validation prompt (Pillar 2: AI-Driven)"""
        
        prompt = f"""
        You are a world-class quality assurance expert with deep expertise in {artifact.artifact_type} assets.
        Your task is to perform rigorous quality validation of this business deliverable.
        
        ARTIFACT TO VALIDATE:
        Name: {artifact.artifact_name}
        Type: {artifact.artifact_type}
        Format: {artifact.content_format}
        Content Length: {len(artifact.content or '') if artifact.content else 0} characters
        Current Quality Score: {artifact.quality_score}
        Business Value Score: {artifact.business_value_score}
        
        ARTIFACT CONTENT:
        {artifact.content[:4000] if artifact.content else "No content available"}...
        
        QUALITY RULE TO APPLY:
        Rule Name: {rule.rule_name}
        Validation Criteria: {rule.ai_validation_prompt}
        Required Threshold: {rule.threshold_score}
        
        VALIDATION FRAMEWORK:
        Evaluate this artifact across these critical dimensions:
        
        1. COMPLETENESS (0.0-1.0):
           - Does it fully address all stated requirements?
           - Are all sections/components present?
           - Is the scope appropriately covered?
        
        2. ACCURACY (0.0-1.0):
           - Is the information factually correct?
           - Are calculations, data, or references accurate?
           - Is the content technically sound?
        
        3. RELEVANCE (0.0-1.0):
           - Does it directly address the business need?
           - Is the content focused and on-target?
           - Does it avoid unnecessary tangents?
        
        4. USABILITY (0.0-1.0):
           - Can stakeholders immediately use this deliverable?
           - Is it well-structured and accessible?
           - Are next steps clear and actionable?
        
        5. PROFESSIONAL STANDARD (0.0-1.0):
           - Does it meet industry professional standards?
           - Is the presentation quality appropriate?
           - Would you be proud to present this to executives?
        
        BUSINESS IMPACT ASSESSMENT:
        - Immediate value: What can stakeholders do with this right now?
        - Short-term impact: How does this advance business objectives?
        - Long-term value: What strategic benefits does this provide?
        - Risk mitigation: What problems does this solve or prevent?
        
        PILLAR COMPLIANCE CHECK:
        Verify compliance with our 14 system pillars:
        1. OpenAI SDK integration - Is AI properly leveraged?
        2. AI-Driven - Does it show AI enhancement and intelligence?
        3. Universal - Is it broadly applicable and language-agnostic?
        4. Scalable - Can this approach scale to larger use cases?
        5. Goal-Driven - Does it clearly advance stated goals?
        6. Memory System - Does it build on previous insights?
        7. Autonomous Pipeline - Can it feed into automated workflows?
        8. Quality Gates - Does it meet quality standards?
        9. Minimal UI/UX - Is the presentation clean and focused?
        10. Real-Time Thinking - Does it show clear reasoning?
        11. Production-Ready - Is it ready for real-world use?
        12. Concrete Deliverables - Is this immediately actionable?
        13. Course-Correction - Can it adapt and improve?
        14. Modular Tools - Is it well-structured and extensible?
        
        RESPONSE FORMAT (JSON):
        {{
            "validation_passed": true,
            "quality_score": 0.87,
            "detailed_feedback": "Comprehensive assessment of quality and areas for improvement",
            "ai_assessment": "AI-driven analysis of strengths and opportunities",
            
            "quality_dimensions": {{
                "completeness_score": 0.90,
                "accuracy_score": 0.85,
                "relevance_score": 0.88,
                "usability_score": 0.82,
                "professional_standard_score": 0.89
            }},
            
            "business_impact_analysis": "Analysis of immediate and strategic business value",
            "actionability_score": 0.85,
            
            "improvement_suggestions": [
                "Specific, actionable suggestion 1",
                "Specific, actionable suggestion 2",
                "Specific, actionable suggestion 3"
            ],
            
            "pillar_compliance": {{
                "compliant_pillars": ["pillar1", "pillar2", "pillar12"],
                "non_compliant_pillars": ["pillar5"],
                "compliance_score": 0.86,
                "compliance_notes": "Detailed compliance assessment"
            }},
            
            "human_review_recommended": false,
            "approval_status": "approved|needs_improvement|requires_human_review",
            
            "quality_gate_decision": {{
                "approved": true,
                "confidence_level": 0.92,
                "decision_reasoning": "Clear explanation of why this decision was made",
                "next_steps": ["what should happen next with this artifact"]
            }},
            
            "processing_time_ms": 1250
        }}
        
        CRITICAL QUALITY STANDARDS:
        - CONCRETE over abstract: Must be immediately usable
        - ACTIONABLE over informational: Must enable decisions/actions  
        - COMPLETE over partial: Must fully address requirements
        - PROFESSIONAL over amateur: Must meet business standards
        - VALUABLE over generic: Must provide clear business benefit
        
        Be rigorous but fair. This artifact will be used in a real business context.
        """
        
        return prompt
    
    async def _make_quality_decision(
        self, 
        artifact: AssetArtifact, 
        validation_results: List[QualityValidation]
    ) -> Dict[str, Any]:
        """Make comprehensive quality decision based on validation results"""
        
        try:
            if not validation_results:
                return {
                    "status": "requires_human_review",
                    "reason": "No validation results available",
                    "overall_score": 0.0,
                    "requires_human_review": True
                }
            
            # Calculate overall quality score (weighted average)
            total_weighted_score = 0.0
            total_weight = 0.0
            
            for validation in validation_results:
                weight = 1.0  # Could be rule-specific weights in the future
                total_weighted_score += validation.score * weight
                total_weight += weight
            
            overall_score = total_weighted_score / total_weight if total_weight > 0 else 0.0
            
            # Count passed validations
            passed_validations = sum(1 for v in validation_results if v.passed)
            pass_rate = passed_validations / len(validation_results)
            
            # Make decision based on multiple criteria
            decision = {
                "overall_score": overall_score,
                "pass_rate": pass_rate,
                "total_validations": len(validation_results),
                "passed_validations": passed_validations,
                "validation_results": validation_results
            }
            
            # Determine status and next steps
            if overall_score >= self.min_quality_score and pass_rate >= 0.8:
                decision.update({
                    "status": "approved",
                    "requires_human_review": False,
                    "reason": f"High quality score ({overall_score:.2f}) and high pass rate ({pass_rate:.1%})"
                })
                
            elif overall_score >= self.human_review_threshold:
                decision.update({
                    "status": "requires_human_review", 
                    "requires_human_review": True,
                    "reason": f"Moderate quality score ({overall_score:.2f}) requires human judgment"
                })
                
            else:
                decision.update({
                    "status": "needs_improvement",
                    "requires_human_review": False,
                    "reason": f"Low quality score ({overall_score:.2f}) needs enhancement"
                })
            
            # Add improvement suggestions
            all_suggestions = []
            for validation in validation_results:
                suggestions = getattr(validation, 'improvement_suggestions', [])
                if suggestions:
                    all_suggestions.extend(suggestions)
            
            decision["improvement_suggestions"] = list(set(all_suggestions))  # Remove duplicates
            
            # Add business impact assessment
            business_impacts = [getattr(v, 'business_impact', None) for v in validation_results]
            business_impacts = [b for b in business_impacts if b]
            if business_impacts:
                decision["business_impact_summary"] = "; ".join(business_impacts)
            
            return decision
            
        except Exception as e:
            logger.error(f"Failed to make quality decision: {e}")
            return {
                "status": "error",
                "error": str(e),
                "requires_human_review": True,
                "overall_score": 0.0
            }
    
    async def _apply_quality_decision(self, artifact: AssetArtifact, decision: Dict[str, Any]):
        """Apply the quality decision to the artifact"""
        
        try:
            status = decision["status"]
            quality_score = decision.get("overall_score", 0.0)
            
            # Update artifact status and quality score
            await update_artifact_status(artifact.id, status, quality_score)
            
            # If approved, trigger goal progress update
            if status == "approved":
                await self._trigger_goal_progress_update(artifact)
            
            # If needs improvement and auto-enhancement enabled, trigger enhancement
            elif status == "needs_improvement" and self.auto_enhancement_enabled:
                await self._trigger_auto_enhancement(artifact, decision)
            
            # If requires human review, create human review task
            elif status == "requires_human_review":
                await self._create_human_review_task(artifact, decision)
            
        except Exception as e:
            logger.error(f"Failed to apply quality decision for artifact {artifact.id}: {e}")
    
    async def _trigger_goal_progress_update(self, artifact: AssetArtifact):
        """Trigger goal progress recalculation when artifact is approved"""
        
        try:
            # This would trigger the goal progress calculation system
            # Implementation depends on having the goal-driven system available
            logger.info(f"ðŸŽ¯ Triggering goal progress update for artifact: {artifact.artifact_name}")
            
            # The actual implementation would call the goal-driven system
            # to recalculate progress based on approved artifacts
            
        except Exception as e:
            logger.error(f"Failed to trigger goal progress update: {e}")
    
    async def _trigger_auto_enhancement(self, artifact: AssetArtifact, decision: Dict[str, Any]):
        """Trigger automated enhancement for artifacts that need improvement"""
        
        try:
            logger.info(f"ðŸ”§ Triggering auto-enhancement for artifact: {artifact.artifact_name}")
            
            # Build enhancement prompt based on validation feedback
            improvement_suggestions = decision.get("improvement_suggestions", [])
            enhancement_prompt = self._build_auto_enhancement_prompt(artifact, improvement_suggestions)
            
            # Use OpenAI SDK for enhancement (Pillar 1: OpenAI SDK)
            response = await self.openai_client.chat.completions.create(
                model=self.quality_validation_model,
                messages=[{"role": "user", "content": enhancement_prompt}],
                response_format={"type": "json_object"},
                temperature=0.2
            )
            
            enhancement_data = json.loads(response.choices[0].message.content)
            
            # Apply enhancements (would require artifact update method)
            # This is a placeholder - actual implementation depends on database methods
            
            logger.info(f"âœ… Auto-enhancement completed for artifact: {artifact.artifact_name}")
            
        except Exception as e:
            logger.error(f"Auto-enhancement failed for artifact {artifact.id}: {e}")
    
    def _build_auto_enhancement_prompt(self, artifact: AssetArtifact, suggestions: List[str]) -> str:
        """Build prompt for automated artifact enhancement"""
        
        return f"""
        You are an expert content enhancer. Improve this artifact based on quality validation feedback.
        
        CURRENT ARTIFACT:
        Name: {artifact.artifact_name}
        Type: {artifact.artifact_type}
        Content: {artifact.content[:2000] if artifact.content else "No content"}...
        
        IMPROVEMENT SUGGESTIONS:
        {chr(10).join(f"- {suggestion}" for suggestion in suggestions)}
        
        ENHANCEMENT OBJECTIVES:
        1. Address all improvement suggestions
        2. Increase business value and actionability
        3. Maintain professional quality standards
        4. Ensure concrete, immediate usability
        
        RESPONSE FORMAT (JSON):
        {{
            "enhanced_content": "Fully improved content",
            "enhancement_summary": "Summary of improvements made",
            "quality_improvements": ["specific improvement 1", "improvement 2"],
            "estimated_quality_score": 0.85
        }}
        """
    
    async def _create_human_review_task(self, artifact: AssetArtifact, decision: Dict[str, Any]):
        """Create human review task for artifacts requiring human judgment"""
        
        try:
            logger.info(f"ðŸ‘¤ Creating human review task for artifact: {artifact.artifact_name}")
            
            # This would integrate with the human feedback system
            # Implementation depends on having human review workflows available
            
            review_data = {
                "artifact_id": artifact.id,
                "artifact_name": artifact.artifact_name,
                "artifact_type": artifact.artifact_type,
                "quality_score": decision.get("overall_score", 0.0),
                "reason": decision.get("reason", ""),
                "improvement_suggestions": decision.get("improvement_suggestions", []),
                "business_impact": decision.get("business_impact_summary", ""),
                "created_at": datetime.utcnow(),
                "status": "pending_review"
            }
            
            # Store human review request (placeholder - needs actual implementation)
            logger.info(f"ðŸ“ Human review task created for artifact: {artifact.artifact_name}")
            
        except Exception as e:
            logger.error(f"Failed to create human review task for artifact {artifact.id}: {e}")
    
    async def _fallback_quality_assessment(self, artifact: AssetArtifact) -> Dict[str, Any]:
        """Fallback quality assessment when no specific rules are available"""
        
        try:
            logger.info(f"âš ï¸ Using fallback quality assessment for artifact: {artifact.artifact_name}")
            
            # Use generic quality assessment prompt
            fallback_prompt = f"""
            Assess the quality of this {artifact.artifact_type} asset using general business standards.
            
            ARTIFACT:
            Name: {artifact.artifact_name}
            Type: {artifact.artifact_type}
            Content: {artifact.content[:2000] if artifact.content else "No content"}...
            
            Rate the quality from 0.0 to 1.0 based on:
            - Completeness and thoroughness
            - Professional presentation
            - Business value and actionability
            - Clarity and usability
            
            RESPONSE FORMAT (JSON):
            {{
                "quality_score": 0.75,
                "assessment": "Quality assessment summary",
                "recommendation": "approved|needs_improvement|requires_human_review"
            }}
            """
            
            response = await self.openai_client.chat.completions.create(
                model=self.quality_validation_model,
                messages=[{"role": "user", "content": fallback_prompt}],
                response_format={"type": "json_object"},
                temperature=0.1
            )
            
            assessment_data = json.loads(response.choices[0].message.content)
            
            return {
                "status": assessment_data.get("recommendation", "requires_human_review"),
                "overall_score": float(assessment_data.get("quality_score", 0.0)),
                "reason": "Fallback assessment - " + assessment_data.get("assessment", ""),
                "requires_human_review": assessment_data.get("recommendation") == "requires_human_review",
                "fallback_assessment": True
            }
            
        except Exception as e:
            logger.error(f"Fallback quality assessment failed: {e}")
            return {
                "status": "error",
                "error": str(e),
                "requires_human_review": True,
                "overall_score": 0.0
            }
    
    async def _update_quality_rules_from_feedback(self, validation_results: List[QualityValidation]):
        """Auto-learning: Update quality rules based on validation feedback (Pillar 4: Auto-apprendente)"""
        
        try:
            if not self.learning_feedback_loop:
                return
            
            logger.info("ðŸ§  Updating quality rules based on validation feedback")
            
            # Analyze patterns in validation results
            # This would involve machine learning to improve quality rules over time
            # Implementation would depend on having ML capabilities and rule update methods
            
            # For now, this is a placeholder for the learning system
            logger.info("âœ… Quality rule learning update completed")
            
        except Exception as e:
            logger.error(f"Failed to update quality rules from feedback: {e}")
    
    async def batch_validate_artifacts(self, artifact_ids: List[UUID]) -> Dict[str, Any]:
        """Batch validate multiple artifacts efficiently"""
        
        results = {
            "processed": 0,
            "approved": 0,
            "needs_improvement": 0,
            "requires_human_review": 0,
            "errors": 0,
            "details": []
        }
        
        try:
            logger.info(f"ðŸ›¡ï¸ Batch validating {len(artifact_ids)} artifacts")
            
            # Process artifacts concurrently (up to configured limit)
            concurrent_limit = int(os.getenv("CONCURRENT_ARTIFACT_PROCESSING", "3"))
            semaphore = asyncio.Semaphore(concurrent_limit)
            
            async def validate_single(artifact_id: UUID):
                async with semaphore:
                    try:
                        # This would require getting artifact from database
                        # Implementation depends on database methods
                        results["processed"] += 1
                        # Update counters based on validation result
                        
                    except Exception as e:
                        results["errors"] += 1
                        logger.error(f"Failed to validate artifact {artifact_id}: {e}")
            
            # Execute batch validation
            await asyncio.gather(*[validate_single(aid) for aid in artifact_ids])
            
            logger.info(f"âœ… Batch validation completed: {results}")
            return results
            
        except Exception as e:
            logger.error(f"Batch validation failed: {e}")
            results["errors"] = len(artifact_ids)
            return results
    
    async def get_quality_metrics_dashboard(self, workspace_id: UUID) -> Dict[str, Any]:
        """Get comprehensive quality metrics for dashboard display"""
        
        try:
            # This would aggregate quality metrics across the workspace
            # Implementation depends on database queries for quality data
            
            metrics = {
                "overall_quality_score": 0.0,
                "artifacts_by_status": {
                    "approved": 0,
                    "needs_improvement": 0,
                    "requires_human_review": 0,
                    "in_progress": 0
                },
                "quality_trends": [],
                "top_improvement_areas": [],
                "pillar_compliance_scores": {},
                "human_review_queue_size": 0
            }
            
            return metrics
            
        except Exception as e:
            logger.error(f"Failed to get quality metrics for workspace {workspace_id}: {e}")
            return {}

    async def get_workspace_quality_metrics(self, workspace_id: UUID) -> Dict[str, Any]:
        """Get quality metrics for a workspace"""
        try:
            # Get quality validations for workspace
            validations_response = self.supabase.table("quality_validations") \
                .select("""
                    *,
                    asset_artifacts!inner(
                        goal_asset_requirements!inner(
                            workspace_goals!inner(workspace_id)
                        )
                    )
                """) \
                .eq("asset_artifacts.goal_asset_requirements.workspace_goals.workspace_id", str(workspace_id)) \
                .execute()
            
            validations = validations_response.data
            
            # Calculate metrics
            total_validations = len(validations)
            passed_validations = len([v for v in validations if v.get("passed", False)])
            avg_score = sum(v.get("score", 0) for v in validations) / total_validations if total_validations > 0 else 0.0
            
            metrics = {
                "total_validations": total_validations,
                "passed_validations": passed_validations,
                "pass_rate": (passed_validations / total_validations) if total_validations > 0 else 0.0,
                "average_quality_score": avg_score,
                "validation_coverage": total_validations  # Simplified metric
            }
            
            logger.info(f"ðŸ“Š Retrieved quality metrics for workspace {workspace_id}")
            return metrics
            
        except Exception as e:
            logger.error(f"Failed to get quality metrics for workspace {workspace_id}: {e}")
            return {
                "total_validations": 0,
                "passed_validations": 0,
                "pass_rate": 0.0,
                "average_quality_score": 0.0,
                "validation_coverage": 0
            }