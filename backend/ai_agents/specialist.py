"""
ðŸš€ SPECIALIST AGENT - PYDANTIC REFACTORED VERSION
Replaces all json.loads with proper Pydantic validation for robustness
Based on specialist.py.BUGGY but with modern error handling
"""

import logging
import os
import json
import asyncio
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
import time
from uuid import UUID

# CRITICAL: Load environment variables for OpenAI API access
from dotenv import load_dotenv
load_dotenv()

logger = logging.getLogger(__name__)

# Pydantic models for validation
from pydantic import BaseModel, Field, ValidationError, validator
from ..models import (
    Agent as AgentModel, 
    Task, 
    TaskStatus, 
    TaskExecutionOutput, 
    AgentStatus
)

# --- Pydantic Models for JSON Parsing ---

class ToolInvocationData(BaseModel):
    """Model for function tool invocation data"""
    class Config:
        extra = "allow"  # Allow additional fields
    
    @classmethod
    def from_json_or_dict(cls, data: Union[str, dict]) -> "ToolInvocationData":
        """Parse from JSON string or dict"""
        if isinstance(data, str):
            try:
                parsed_data = json.loads(data)
                return cls(**parsed_data) if isinstance(parsed_data, dict) else cls()
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse tool data as JSON: {data}")
                return cls()
        return cls(**data) if isinstance(data, dict) else cls()

class TaskExecutionResult(BaseModel):
    """Model for task execution results"""
    task_id: str
    status: str = "completed"
    summary: Optional[str] = None
    detailed_results_json: Optional[str] = None
    result: Optional[str] = None
    deliverable_content: Optional[Any] = None
    
    @validator('detailed_results_json', pre=True)
    def stringify_detailed_results(cls, v):
        """Ensure detailed_results_json is a string"""
        if v and not isinstance(v, str):
            return json.dumps(v, default=str)
        return v

class ExecutionLogDetails(BaseModel):
    """Model for execution log details"""
    raw_details: Optional[str] = None
    timestamp: Optional[str] = None
    agent_id: Optional[str] = None
    agent_name: Optional[str] = None
    agent_role: Optional[str] = None
    event_step: Optional[str] = None
    workspace_id: Optional[str] = None
    current_task_id_context: Optional[str] = None
    
    class Config:
        extra = "allow"
    
    @classmethod
    def from_json_or_dict(cls, data: Union[str, dict]) -> "ExecutionLogDetails":
        """Parse from JSON string or dict"""
        if isinstance(data, str):
            try:
                parsed_data = json.loads(data)
                return cls(**parsed_data) if isinstance(parsed_data, dict) else cls(raw_details=data)
            except json.JSONDecodeError:
                return cls(raw_details=data)
        return cls(**data) if isinstance(data, dict) else cls()

class ChunkedCompletionData(BaseModel):
    """Model for chunked completion response"""
    summary: str = "Completed via chunked strategy"
    deliverable_content: Optional[Any] = None
    task_name: Optional[str] = None
    result: Optional[str] = None
    
    class Config:
        extra = "allow"

# Configure OpenAI trace BEFORE importing SDK
if os.getenv('OPENAI_TRACE', 'false').lower() == 'true':
    logger.info("ðŸ” Configuring OpenAI trace for specialist agent")
    os.environ['OPENAI_TRACE'] = 'true'
    os.environ['OPENAI_TRACE_SAMPLE_RATE'] = os.getenv('OPENAI_TRACE_SAMPLE_RATE', '1.0')
    os.environ['OPENAI_TRACE_INCLUDE_SYSTEM'] = os.getenv('OPENAI_TRACE_INCLUDE_SYSTEM', 'true')
    os.environ['OPENAI_TRACE_INCLUDE_TOOLS'] = os.getenv('OPENAI_TRACE_INCLUDE_TOOLS', 'true')

# Import SDK components
try:
    from agents import (
        Agent as OpenAIAgent, 
        Runner,
        function_tool as sdk_function_tool,
        WebSearchTool,
        FileSearchTool,
        handoff,
        RunContextWrapper,
        RunResult,
        input_guardrail,
        output_guardrail,
        GuardrailFunctionOutput
    )
    SDK_AVAILABLE = True
    logger.info("âœ… OpenAI Agents SDK loaded successfully with trace configuration")
except ImportError:
    SDK_AVAILABLE = False
    logger.warning("OpenAI Agents SDK not available - using minimal fallback")
    
    # Fallback implementations with Pydantic validation
    import types
    
    def function_tool(func=None, *a, **kw):
        def decorator(f):
            async def on_invoke_tool(_c, p):
                # Use Pydantic for parsing instead of raw json.loads
                tool_data = ToolInvocationData.from_json_or_dict(p)
                if asyncio.iscoroutinefunction(f):
                    return await f(**tool_data.dict())
                return f(**tool_data.dict())

            return types.SimpleNamespace(on_invoke_tool=on_invoke_tool)

        return decorator if func is None else decorator(func)
    
    # Alias for compatibility
    sdk_function_tool = function_tool

    class WebSearchTool:
        pass

    class FileSearchTool:
        pass

    class AgentOutputSchema:
        def __init__(self, schema_class, strict_json_schema=True):
            self.schema_class = schema_class

# Import database operations
from ..database import (
    update_task_status,
    create_handoff,
    get_task,
    get_agent,
    list_agents
)

# Import other components
from ..services.unified_memory_engine import unified_memory_engine
# from .conversational import ConversationalAgent # This is imported inside the method to avoid circular dependency

# Configuration
try:
    from ..config.agent_system_config import AgentSystemConfig
    config = AgentSystemConfig()
except ImportError:
    class AgentSystemConfig:
        def __init__(self):
            self.SPECIALIST_AGENT_TIMEOUT = 150
            self.ENABLE_AI_AGENT_MATCHING = os.getenv('ENABLE_AI_AGENT_MATCHING', 'true').lower() == 'true'
            self.ENABLE_TRACE_LOGGING = os.getenv('ENABLE_TRACE_LOGGING', 'false').lower() == 'true'
    config = AgentSystemConfig()

# Robust JSON parser with Pydantic fallback
def parse_llm_json_robust_pydantic(
    raw_output: str, 
    task_id: str = None, 
    expected_model: BaseModel = TaskExecutionResult
):
    """Parse LLM output using Pydantic models with fallback"""
    try:
        # Try to parse as JSON first
        raw_data = json.loads(raw_output)
        # Validate with Pydantic
        validated = expected_model(**raw_data)
        return validated.dict(), True, "pydantic_parse"
    except json.JSONDecodeError:
        # Try to extract JSON from markdown or other formats
        import re
        json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', raw_output, re.DOTALL)
        if json_match:
            try:
                extracted_data = json.loads(json_match.group(1))
                validated = expected_model(**extracted_data)
                return validated.dict(), True, "pydantic_markdown_extract"
            except:
                pass
    except ValidationError as e:
        logger.warning(f"Pydantic validation failed: {e}")
    
    # Fallback: create minimal valid object
    fallback_data = {
        "task_id": task_id or "unknown",
        "status": "failed",
        "summary": "JSON parsing failed - using fallback"
    }
    try:
        validated = expected_model(**fallback_data)
        return validated.dict(), False, "pydantic_fallback"
    except:
        return fallback_data, False, "raw_fallback"

class SpecialistAgent:
    """Enhanced Specialist Agent with Pydantic validation"""
    
    def __init__(self, agent_data: AgentModel):
        self.agent_data = agent_data
        self.logger = logging.getLogger(f"SpecialistAgent-{agent_data.name}")
        self.execution_timeout = config.SPECIALIST_AGENT_TIMEOUT
        self._current_task_being_processed_id = None
        
        # Initialize SDK agent if available
        self.sdk_agent = None
        self.runner = None
        
        if SDK_AVAILABLE:
            self._initialize_sdk_agent()
    
    def _initialize_sdk_agent(self):
        """Initialize OpenAI SDK agent with enhanced configuration"""
        try:
            # Build tool configuration
            tools = []
            tool_instances = {}
            
            # Add web search if enabled
            if self._has_tool_access("web_search"):
                tools.append(WebSearchTool())
                
            # Add file search if enabled  
            if self._has_tool_access("file_search"):
                tools.append(FileSearchTool())
                
            # Add custom function tools
            custom_tools = self.agent_data.tools or []
            for tool_config in custom_tools:
                if tool_config.get("type") == "function":
                    tool_name = tool_config.get("name")
                    if tool_name:
                        # Create dynamic function tool with Pydantic validation
                        tool_func = self._create_function_tool(tool_name, tool_config)
                        if tool_func:
                            tool_instances[tool_name] = tool_func
                            tools.append(tool_func)
            
            # Create SDK agent
            self.sdk_agent = OpenAIAgent(
                name=self.agent_data.name,
                system_prompt=self._build_system_prompt(),
                model=self._get_model_config(),
                tools=tools
            )
            
            # Create runner
            self.runner = Runner()
            
            logger.info(f"âœ… SDK agent initialized for {self.agent_data.name} with {len(tools)} tools")
            
        except Exception as e:
            logger.error(f"Failed to initialize SDK agent: {e}")
            self.sdk_agent = None
            self.runner = None
    
    def _create_function_tool(self, tool_name: str, tool_config: dict):
        """Create a function tool with Pydantic validation"""
        @sdk_function_tool
        async def dynamic_tool(**kwargs):
            # Validate input with Pydantic
            tool_data = ToolInvocationData(**kwargs)
            
            # Execute tool logic
            result = await self._execute_custom_tool(tool_name, tool_data.dict())
            return result
        
        # Set metadata
        dynamic_tool.__name__ = tool_name
        dynamic_tool.__doc__ = tool_config.get("description", f"Custom tool: {tool_name}")
        
        return dynamic_tool
    
    async def _log_execution_internal(
        self, step: str, details: Union[str, Dict]
    ) -> bool:
        """Log execution with Pydantic validation"""
        try:
            # Parse details with Pydantic
            log_details = ExecutionLogDetails.from_json_or_dict(details)
            
            # Add agent context
            log_details.timestamp = datetime.now().isoformat()
            log_details.agent_id = str(self.agent_data.id)
            log_details.agent_name = self.agent_data.name
            log_details.agent_role = self.agent_data.role
            log_details.event_step = step
            log_details.workspace_id = str(self.agent_data.workspace_id)
            log_details.current_task_id_context = self._current_task_being_processed_id
            
            log_entry = log_details.dict()
            
            # Log to memory system
            if hasattr(self, 'workspace_id'):
                await unified_memory_engine.store_agent_execution_log(
                    self.workspace_id,
                    str(self.agent_data.id),
                    log_entry
                )
                
            self.logger.info(f"[{step}] {log_entry}")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to log execution: {e}")
            return False
    
    async def execute_task(self, task: Task) -> TaskExecutionOutput:
        """Execute task with Pydantic validation throughout"""
        start_time = time.time()
        self._current_task_being_processed_id = str(task.id)
        
        try:
            await self._log_execution_internal("task_start", {
                "task_id": str(task.id),
                "task_name": task.name,
                "task_description": task.description
            })
            
            # Check for SDK availability
            if SDK_AVAILABLE and self.sdk_agent and self.runner:
                # Use SDK execution path
                result = await self._execute_with_sdk(task)
            else:
                # Use fallback execution
                result = await self._execute_fallback(task)
            
            # Validate result with Pydantic
            if isinstance(result, dict):
                execution_output = TaskExecutionOutput(**result)
            else:
                execution_output = result
                
            execution_output.execution_time = time.time() - start_time
            
            await self._log_execution_internal("task_complete", {
                "task_id": str(task.id),
                "status": execution_output.status,
                "execution_time": execution_output.execution_time
            })
            
            return execution_output
            
        except Exception as e:
            logger.error(f"Task execution failed: {e}")
            return TaskExecutionOutput(
                task_id=task.id,
                status=TaskStatus.FAILED,
                error_message=str(e),
                execution_time=time.time() - start_time
            )
    
    async def _execute_with_sdk(self, task: Task) -> Dict[str, Any]:
        """Execute task using SDK with Pydantic validation"""
        try:
            # Prepare prompt
            prompt = self._build_task_prompt(task)
            
            # Run with SDK
            run_result = await self.runner.run(
                agent=self.sdk_agent,
                messages=[{"role": "user", "content": prompt}],
                max_steps=10,
                context_variables={"task_id": str(task.id)}
            )
            
            # Extract and validate result
            if run_result and hasattr(run_result, 'messages'):
                last_message = run_result.messages[-1] if run_result.messages else None
                if last_message and hasattr(last_message, 'content'):
                    raw_output = last_message.content
                    
                    # Parse with Pydantic
                    parsed_data, success, method = parse_llm_json_robust_pydantic(
                        raw_output, str(task.id), TaskExecutionResult
                    )
                    
                    # Process and return
                    processed_data = self._preprocess_task_output_data(parsed_data, task)
                    return processed_data
            
            # Fallback if no valid output
            return {
                "task_id": str(task.id),
                "status": "completed",
                "summary": "Task completed via SDK",
                "result": "SDK execution completed without structured output"
            }
            
        except Exception as e:
            logger.error(f"SDK execution failed: {e}")
            raise
    
    async def _execute_fallback(self, task: Task) -> Dict[str, Any]:
        """Fallback execution with Pydantic validation"""
        try:
            from openai import OpenAI
            client = OpenAI()
            
            prompt = self._build_task_prompt(task)
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=self._get_model_config(),
                    messages=[
                        {"role": "system", "content": self._build_system_prompt()},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    max_tokens=2000,
                    response_format={"type": "json_object"}
                ),
                timeout=self.execution_timeout
            )
            
            ai_output = response.choices[0].message.content
            
            # Parse and validate with Pydantic
            try:
                result_data = TaskExecutionResult.model_validate_json(ai_output)
                return result_data.dict()
            except ValidationError:
                # Create structured fallback
                fallback_result = TaskExecutionResult(
                    task_id=str(task.id),
                    status="completed",
                    summary=f"Task {task.name} completed using fallback execution",
                    detailed_results_json=json.dumps({
                        "deliverable_content": ai_output,
                        "task_name": task.name,
                        "execution_method": "fallback"
                    })
                )
                return fallback_result.dict()
                
        except Exception as e:
            logger.error(f"Fallback execution failed: {e}")
            raise
    
    async def _execute_chunked_completion(self, task: Task) -> Optional[Dict[str, Any]]:
        """Execute task with chunked completion strategy using Pydantic"""
        try:
            # ... chunked execution logic ...
            
            completion_result = None  # Placeholder for actual implementation
            
            if completion_result and hasattr(completion_result, 'final_output'):
                # Parse with Pydantic
                try:
                    completion_data = ChunkedCompletionData.model_validate_json(
                        completion_result.final_output
                    )
                    
                    # Create successful result
                    chunked_result = TaskExecutionOutput(
                        task_id=str(task.id),
                        status="completed",
                        summary=completion_data.summary,
                        detailed_results_json=json.dumps(completion_data.dict()),
                    )
                    
                    return chunked_result.dict()
                    
                except ValidationError as e:
                    logger.warning(f"Failed to parse chunked completion: {e}")
                    return None
                    
        except Exception as e:
            logger.error(f"Chunked completion failed: {e}")
            return None
    
    def _preprocess_task_output_data(self, parsed_data: dict, task: Task) -> dict:
        """Preprocess task output data"""
        # Ensure required fields
        if "task_id" not in parsed_data:
            parsed_data["task_id"] = str(task.id)
            
        if "status" not in parsed_data:
            parsed_data["status"] = "completed"
            
        # Convert status to TaskStatus enum value
        if isinstance(parsed_data.get("status"), str):
            status_map = {
                "completed": TaskStatus.COMPLETED.value,
                "failed": TaskStatus.FAILED.value,
                "in_progress": TaskStatus.IN_PROGRESS.value,
                "pending": TaskStatus.PENDING.value
            }
            parsed_data["status"] = status_map.get(
                parsed_data["status"].lower(), 
                TaskStatus.COMPLETED.value
            )
        
        return parsed_data
    
    def _build_system_prompt(self) -> str:
        """Build system prompt for the agent"""
        return f"""You are {self.agent_data.name}, a {self.agent_data.seniority} {self.agent_data.role}.
{self.agent_data.description if self.agent_data.description else ''}

Your primary function is to execute tasks and produce concrete, final deliverables. 
- **DO NOT** provide plans, instructions, or explanations on how to do the task.
- **DO** generate the final, complete artifact as requested by the task.
- If the task is to "write code", you must write the complete, functional code.
- If the task is to "create a list", you must generate the list in the specified format.
- Your output must be the deliverable itself, not a description of it.

IMPORTANT: Always respond with a valid JSON object containing the final deliverable content."""
    
    def _build_task_prompt(self, task: Task) -> str:
        """Build task-specific prompt"""
        return f"""Execute the following task:

Task ID: {task.id}
Task Name: {task.name}
Description: {task.description}

Respond with a JSON object containing:
- task_id: The task ID
- status: "completed" or "failed"
- summary: Brief summary of what was done
- detailed_results_json: Detailed results as a JSON string
- result: Main result content"""
    
    def _get_model_config(self) -> str:
        """Get model configuration"""
        if self.agent_data.llm_config:
            return self.agent_data.llm_config.get("model", "gpt-4o-mini")
        return "gpt-4o-mini"
    
    def _has_tool_access(self, tool_type: str) -> bool:
        """Check if agent has access to a specific tool type"""
        if not self.agent_data.tools:
            return False
        return any(
            tool.get("type") == tool_type 
            for tool in self.agent_data.tools
        )
    
    async def _execute_custom_tool(self, tool_name: str, tool_data: dict) -> Any:
        """Execute a custom tool"""
        # Placeholder for custom tool execution
        logger.info(f"Executing custom tool {tool_name} with data: {tool_data}")
        return {"status": "success", "tool": tool_name}

# Export the refactored agent
__all__ = ['SpecialistAgent']